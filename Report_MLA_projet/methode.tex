%\textbf{Une présentation synthétique de la solution ré-implémentée
%de l’article. Précisez et justifiez les éventuelles
%différences avec l'article de référence}\\

%\lipsum[2]

Researchers have identified a serious security concern with existing neural network models: an attacker can easily fool a neural network by adding specific noise to benign samples, often undetected. The attacker uses perturbations that are not perceptible to human vision/audition, which are sufficient to cause a normally trained model to output false predictions with high confidence, a phenomenon that researchers call adversarial attacks.
\\

Existing adversarial attacks can be classified as white-box, grey-box and black-box attacks based on the threat model. The difference between these three models lies in the information known to the attacker, and the FGSM approach is a white-box attack in which the threat model assumes that the attacker has complete knowledge of his target model, including the model architecture and parameters. The attacker can therefore create an adversarial sample directly on the target model by any means. \\

FGSM is a typical one-step attack algorithm who performs a one-step update along the direction of the gradient of the adversarial loss function $J(\theta, x, y)$(i.e. the sign) to increase the loss in the steepest direction.\\

When training the classification model, the network learns the features based on the input image, and finally goes through the activation function layer to obtain the probability, calculates the loss value through the loss function, and passes back the gradient (gradient back propagation), and finally the network is updated based on the gradient, the purpose of the update is to make the loss value smaller and smaller, so that the model classification probability is also higher and higher.\\

The aim of the FGSM attack is to disrupt the classification of the network by modifying the pixel values of the input images without modifying the network parameters. Based on the above principles of neural network computation, the loss values can be passed back to the image and the gradient $J(\theta, x, y)$as well as the direction of the gradient $sign(J(\theta, x, y))$can be calculated.The reason for using the gradient direction rather than the gradient value is to control the size of the perturbation.\\

The classification model updates the parameters by subtracting the gradient from the parameters so that the loss value is reduced and the probability of a correct prediction is increased. Since the attack is intended to make the network misclassify the images, it is only necessary to make the loss increase. Therefore, adding the direction of the gradient to the input image to increase the value of the loss is sufficient to interfere with the network's classification. This is the principle of the FGSM algorithm, which calculates the gradient based on the image and adds the gradient when updating the image.\\
\begin{equation}
    x' = x + \epsilon \cdot sign(\nabla_{x}J(\theta, x, y))
\end{equation}
$\eta = sign(\nabla_{x}J(\theta, x, y))$is the perturbation, $\epsilon$ is the weighting parameter that can be used to control the magnitude of the attack. 