
\begin{abstract}

In the era of data computing driven by deep learning algorithms, it is crucial to ensure the security and robustness of the algorithms : some machine learning models consistently misclassify adversarial examples. Szegedy et al. (2014b) argue that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature, and uses this view to generate a simple and fast method of generating adversarial examples - FGSM. In this project, we focus on generating adversarial samples and confirming their impact on neural networks using the FGSM method, and implementing adversarial training of linear models as well as deep networks. We work on generating adversarial samples on different neural networks and observe how the samples interfere with the classification of the neural network. After the adversarial training is completed, we compare the robustness of the neural network to the adversarial interference before and after training and confirm the effectiveness of the adversarial training.We will discuss the ability of different architectures of neural networks to resist interference. In addition,we will discuss the ability of different architectures of neural networks to resist interference. And, we will also study...


%\textbf{Le résumé  synthétise en environ 200 mots la tâche abordées, les problèmes associés, la solution proposée, et les résultats principaux} \\

\end{abstract}

\begin{keywords}
Adversarial examples, FGSM,  Adversarial trainin, GoogLeNet, Maxout, DNN,etc... 
\end{keywords}
