
\begin{abstract}

 \cite{szegedy2013intriguing} argue that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature, and uses this view to generate a simple and fast method of generating adversarial examples - FGSM. In this project, we focus on generating adversarial examples and confirming their impact on neural networks using the FGSM method, and implementing adversarial training of linear models as well as deep networks. After the adversarial training is completed, we compare the robustness of the neural network to the adversarial interference before and after training and confirm the effectiveness of the adversarial training.In addition,we will discuss the ability of different architectures of neural networks to resist interference.
 
\end{abstract}

\begin{keywords}
Adversarial examples, FGSM,  Adversarial training, GoogLeNet, Maxout,  Linear Model.
\end{keywords}
