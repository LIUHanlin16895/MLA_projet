
\textbf{Conclusion : The paper "Explaining and Harnessing Adversarial Examples" presents a new explanation for the vulnerability of machine learning models, including neural networks, to adversarial examples. By reproducing the results, we can find that the primary cause of this vulnerability is the linear nature of these models, and the results support this explanation. The authors also propose a method for generating adversarial examples and show that using these examples for adversarial training can improve the performance of a maxout network on the MNIST dataset. Overall, the paper provides a new perspective on the phenomenon of adversarial examples and offers a potential solution for reducing their impact.} \\

%\lipsum[7]