
%\textbf{L'introduction présente le contexte général lié à la tâche, description de la tâche à traiter, détail des problèmes qui y sont associés.} \\

%\lipsum[2]

\cite{szegedy2013intriguing} argue that the vulnerability of machine learning models to interference from adversarial examples is due to the extreme non-linearity of deep neural networks. In line with this view, Szegedy et al.devise a fast method for generating adversarial examples, FGSM, and show that adversarial training can indeed greatly improve the robustness and accuracy of neural networks. In their study, they first explain the existence of adversarial examples for linear models, and explain the principle of FGSM. Afterwards, Szegedy et al. implement adversarial training on a variety of linear models and deep networks, and demonstrate the effectiveness and practicality of the training. 

The aim of this project is to re-implement the algorithms in the paper and to reproduce all the experimental results obtained. We have approached the reproduction of the paper in three main directions: firstly, the existence of adversarial examples. Second, the impact of adversarial attacks. Third, the practicality of adversarial training.

To do this, we first need to understand the adversarial example and its existence: we study its rationale and its generation method, FGSM, and generate an adversarial example based on a neural network model. Secondly, we look at the effect of this adversarial example on linear neural network models, in particular logistic regression networks (\textit{simple linear neural networks, softmax, etc.}), and implement the adversarial example training in these networks to see the effectiveness of the adversarial training. Thirdly, we will focus on adversarial training of deep networks and try to demonstrate that deep networks are more robust to adversarial examples than simple linear neural networks. In this section, we will work on generating adversarial examples and implementing adversarial training on Maxout, etc. Fourthly, we will compare the robustness of the adversarial examples and the results of the adversarial training on the above different architectures and hope to try more different approaches based on the adversarial training, such as  \textit{early stopping} and expanding the model to improve the accuracy of the model.\\